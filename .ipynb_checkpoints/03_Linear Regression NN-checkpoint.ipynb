{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e71e76c",
   "metadata": {},
   "source": [
    "# Linear Regression Netural Network"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a292fc9",
   "metadata": {},
   "source": [
    "For example:\n",
    "y = a*x + b*x2 + c;\n",
    "a, b are the weight"
   ]
  },
  {
   "cell_type": "raw",
   "id": "812e28e8",
   "metadata": {},
   "source": [
    "In deep learning, gradient descent is a fundamental optimization algorithm used to minimize the loss function of a neural network model during the training process. Here's an overview of how gradient descent works in the context of deep learning:\n",
    "\n",
    "Loss Function: In deep learning, we define a loss function that quantifies how well the model's predictions match the actual target values. The goal of training is to minimize this loss function.\n",
    "\n",
    "Gradient Calculation: Gradient descent works by iteratively adjusting the parameters of the neural network to minimize the loss function. To do this, we need to calculate the gradient of the loss function with respect to each model parameter. This gradient represents the direction and magnitude of the steepest increase of the loss function.\n",
    "\n",
    "Parameter Update: Once we have the gradient, we update the parameters of the neural network in the opposite direction of the gradient to reduce the loss. This is done by taking steps proportional to the negative of the gradient.\n",
    "\n",
    "Learning Rate: The size of the steps taken during each parameter update is controlled by a hyperparameter called the learning rate. It determines the step size and affects the convergence of the optimization process.\n",
    "\n",
    "Iterations: This process is repeated iteratively for a fixed number of iterations or until a convergence criterion is met, such as when the change in loss becomes negligible.\n",
    "\n",
    "There are different variants of gradient descent, such as stochastic gradient descent (SGD), mini-batch gradient descent, and Adam optimizer, each with its own variations in how the gradients are computed and how the parameters are updated. However, the core principle of iteratively updating parameters based on the gradient of the loss function remains the same across these variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b38780",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e151f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer: #@save\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c4dee1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[43mn\u001b[49m)\n\u001b[0;32m      2\u001b[0m timer \u001b[38;5;241m=\u001b[39m Timer()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "c = torch.zeros(n)\n",
    "timer = Timer()\n",
    "for i in range(n):\n",
    "    c[i] = a[i] + b[i]\n",
    "f'{timer.stop():.5f} sec'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485db3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
